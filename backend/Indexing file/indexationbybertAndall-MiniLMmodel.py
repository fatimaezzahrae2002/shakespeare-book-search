# -*- coding: utf-8 -*-
"""indexationByBertModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lo5olEp1z8iOrWqBf-7jGwiRHfyvrivh
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install PyMuPDF
!pip install sentence_transformers

import os
import json
import fitz
import re
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/bert-base-nli-mean-tokens")

dossier_articles = "/content/drive/MyDrive/ressources"
nom_fichier_index = "/content/drive/MyDrive/index_bert_pages.json"
index_vecteurs = {}

def nettoyer_texte(texte):
    texte = texte.lower()
    texte = re.sub(r'[^a-zA-Z0-9\s]', ' ', texte)
    texte = re.sub(r'\s+', ' ', texte)
    return texte.strip()

if os.path.exists(nom_fichier_index):
    with open(nom_fichier_index, "r", encoding="utf-8") as f:
        index_vecteurs = json.load(f)
else:
    fichiers = [f for f in os.listdir(dossier_articles) if f.endswith(".pdf")]

    for fichier in fichiers:
        chemin = os.path.join(dossier_articles, fichier)

        try:
            pdf = fitz.open(chemin)
        except Exception as e:
            print(f"Error reading {fichier}: {e}")
            continue

        print(f"Indexing {fichier} ({pdf.page_count} pages)...")

        for page_num, page in enumerate(pdf, start=1):
            texte_page = page.get_text()
            texte_nettoye = nettoyer_texte(texte_page)

            if texte_nettoye.strip() == "":
                continue
            # Embedding MiniLM
            vecteur = model.encode(texte_nettoye).tolist()

            # Stocker avec cl√© "fichier_page"
            key = f"{fichier}_page_{page_num}"
            index_vecteurs[key] = vecteur

            print(f"Indexed: {key}")

        pdf.close()

    # Sauvegarder index
    with open(nom_fichier_index, "w", encoding="utf-8") as f:
        json.dump(index_vecteurs, f, indent=2, ensure_ascii=False)

print(f"MiniLM index ready! {len(index_vecteurs)} page vectors embedded.")

import os
import json
import fitz
import re
from sentence_transformers import SentenceTransformer

# === Charger mod√®le ===
model = SentenceTransformer("sentence-transformers/bert-base-nli-mean-tokens")

# === Dossiers ===
dossier_articles = "/content/drive/MyDrive/ressources"
nom_fichier_index_livres = "/content/drive/MyDrive/index_bert_books.json"

# === Fonctions utilitaires ===
def nettoyer_texte(texte):
    texte = texte.lower()
    texte = re.sub(r'[^a-zA-Z0-9\s]', ' ', texte)
    texte = re.sub(r'\s+', ' ', texte)
    return texte.strip()

# === Index final (un vecteur par livre) ===
index_livres = {}

# ====== INDEXATION ======
fichiers = [f for f in os.listdir(dossier_articles) if f.endswith(".pdf")]

for fichier in fichiers:
    chemin = os.path.join(dossier_articles, fichier)

    print(f"\nüìò Traitement du livre : {fichier}")

    try:
        pdf = fitz.open(chemin)
    except Exception as e:
        print(f"Impossible d‚Äôouvrir {fichier}: {e}")
        continue

    # Concat√©ner toutes les pages du PDF
    texte_total = ""
    for page in pdf:
        texte_page = page.get_text()
        texte_total += " " + texte_page

    pdf.close()

    # Nettoyage du texte
    texte_nettoye = nettoyer_texte(texte_total)

    if texte_nettoye.strip() == "":
        print(f"‚ö†Ô∏è Livre vide : {fichier}")
        continue

    # Embedding du livre entier
    print("   ‚Üí G√©n√©ration embedding...")
    vecteur = model.encode(texte_nettoye).tolist()

    # Enregistrer dans l‚Äôindex
    index_livres[fichier] = vecteur

print("\nüíæ Sauvegarde des embeddings...")
with open(nom_fichier_index_livres, "w", encoding="utf-8") as f:
    json.dump(index_livres, f, indent=2, ensure_ascii=False)

print(f"\n‚úÖ Indexation termin√©e : {len(index_livres)} livres index√©s !")

import json
import torch
from sentence_transformers import SentenceTransformer, util

# ===== Charger MiniLM =====
model = SentenceTransformer("sentence-transformers/bert-base-nli-mean-tokens")

# ===== Charger l'index =====
with open("/content/drive/MyDrive/index_minilm_pages.json", "r", encoding="utf-8") as f:
    index_vecteurs_raw = json.load(f)

# Convertir tous les vecteurs JSON en torch.float32 pour acc√©l√©rer la recherche
index_vecteurs = {}
for doc, vec in index_vecteurs_raw.items():
    index_vecteurs[doc] = torch.tensor(vec, dtype=torch.float32)

print(f"Loaded {len(index_vecteurs)} document vectors.")

# ===== Recherche =====
def search(query, top_k=20):
    # Encoder la requ√™te en torch.float32
    q_vec = model.encode(query, convert_to_tensor=True).float()

    results = []
    for doc, vec_t in index_vecteurs.items():
        # Similarit√© cosinus
        score = util.cos_sim(q_vec, vec_t).item()
        if score >= 0.0:

            results.append((doc, score))

    # Trier par score d√©croissant
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]

# ===== Exemple d'utilisation =====
if __name__ == "__main__":
    query = input("Enter your query: ")
    results = search(query)

    print("\nTop results:")
    for doc, score in results:
        print(f"{doc} ‚Äî {score:.4f}")

def book_title(filename):
    try:
        clean_name = filename.replace(".pdf", "")
        if " author " in clean_name:
            title_part = clean_name.split(" author ")[0]
        else:
            title_part = clean_name

        words = title_part.split()

        if words and words[0].isdigit():
            words = words[1:]  # On garde tout sauf le 1er √©l√©ment
        return " ".join(words)

    except Exception as e:
        return None

import os
import json
import requests
DOSSIER_PDF = "/content/drive/MyDrive/ressources"
DOSSIER_META = "/content/drive/MyDrive/MesLivresMetadata"
os.makedirs(DOSSIER_META, exist_ok=True)
def chercher_metadonnees(titre):
    if not titre:
        return None

    url = "https://www.googleapis.com/books/v1/volumes"
    params = {"q": titre, "maxResults": 1} # On ne veut que le 1er r√©sultat

    try:
        response = requests.get(url, params=params)
        data = response.json()

        # CORRECTION DU BUG "KeyError: items"
        if "items" not in data:
            print(f"   [API] Rien trouv√© pour : {titre}")
            return None

        return data["items"][0]["volumeInfo"]

    except Exception as e:
        print(f"   [Erreur API] {e}")
        return None

# --- COEUR DU PROGRAMME ---
print("--- D√©but du traitement ---")

# Lister tous les fichiers du dossier
fichiers = [f for f in os.listdir(DOSSIER_PDF) if f.endswith(".pdf")]

for fichier in fichiers:
    print(f"\nTraitement de : {fichier}")

    # 1. Extraire le titre propre
    titre_propre = book_title(fichier)
    print(f"   Titre d√©tect√© : {titre_propre}")

    # 2. Chercher les infos
    meta = chercher_metadonnees(titre_propre)

    if meta:
        # 3. Sauvegarder en JSON
        # On garde le m√™me nom que le PDF mais en .json
        nom_json = fichier.replace(".pdf", ".json")
        chemin_json = os.path.join(DOSSIER_META, nom_json)

        with open(chemin_json, 'w', encoding='utf-8') as f:
            json.dump(meta, f, ensure_ascii=False, indent=4)

        print(f"   ‚úÖ Sauvegard√© dans : {nom_json}")
    else:
        print("   ‚ùå Pas de m√©tadonn√©es trouv√©es.")

print("\n--- Termin√© ! ---")

import os, json, re
import fitz
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import numpy as np

#MODEL_NAME = "sentence-transformers/bert-base-nli-mean-tokens"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
model = SentenceTransformer(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

dossier = "/content/drive/MyDrive/ressources"
out_path = "/content/drive/MyDrive/index_books_chunks.json"

# param√®tres de chunking (adaptables)
MAX_TOKENS = min(tokenizer.model_max_length, 512)  # s√©curit√©
CHUNK_SIZE = 450
OVERLAP = 50

def nettoyer_texte(t):
    t = t.replace("\n", " ")
    t = re.sub(r'\s+', ' ', t)
    return t.strip()

def chunk_text_by_tokens(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):
    toks = tokenizer(text, add_special_tokens=False)["input_ids"]
    chunks = []
    start = 0
    L = len(toks)
    while start < L:
        end = min(start + chunk_size, L)
        chunk_ids = toks[start:end]
        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        chunks.append((start, end, chunk_text))
        if end == L:
            break
        start = end - overlap
    return chunks

index = {}

pdfs = [f for f in os.listdir(dossier) if f.lower().endswith(".pdf")]
for pdf_file in pdfs:
    path = os.path.join(dossier, pdf_file)
    print("Processing", pdf_file)
    try:
        pdf = fitz.open(path)
    except Exception as e:
        print("Error opening", pdf_file, e)
        continue

    # concat toutes les pages (ou tu peux chunker page par page si tu pr√©f√®res)
    full_text = ""
    page_offsets = []  # pour retrouver de quelle page vient chaque chunk (facultatif)
    for i, page in enumerate(pdf, start=1):
        txt = page.get_text()
        txt = nettoyer_texte(txt)
        if txt:
            # stocke offset de caract√®re approximatif
            page_offsets.append((len(full_text), i))
            full_text += " " + txt

    pdf.close()

    if not full_text.strip():
        continue

    # d√©couper en chunks par tokens
    chunks = chunk_text_by_tokens(full_text, CHUNK_SIZE, OVERLAP)
    chunk_texts = [c[2] for c in chunks]

    # encoder par batch pour vitesse
    embeddings = model.encode(chunk_texts, convert_to_numpy=True, show_progress_bar=True)

    # agr√©gation: moyenne de tous les chunks (ou moyenne top-k)
    agg_embedding = embeddings.mean(axis=0)

    # construire structure √† sauvegarder
    index[pdf_file] = {
        "agg_embedding": agg_embedding.tolist(),
        "chunks": [
            {
                "start_token": int(start),
                "end_token": int(end),
                "text_preview": (chunk_texts[idx][:300] + "...") if len(chunk_texts[idx])>300 else chunk_texts[idx],
                "page_guess": None  # optionnel: on peut approximer la page en fonction des offsets
            }
            for idx, (start, end, _ ) in enumerate(chunks)
        ],
        # on sauvegarde embeddings des chunks s√©par√©ment (attention taille du fichier)
        "chunk_embeddings": [emb.tolist() for emb in embeddings]
    }

# sauvegarde
with open(out_path, "w", encoding="utf-8") as f:
    json.dump(index, f, ensure_ascii=False, indent=2)

print("Saved index to", out_path)